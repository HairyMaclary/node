[for inclusion into other notes:

Function Constructors

A function constructor is a normal function that is used to create objects.  If you use the new keyword the 'this' variable points to a new empty object, and that object is returned from the function automatically. 

Because 'this' has been automaticaly created then we can attach things to it inside the function before it is returned:

    function Person (firstName, lastName) {
        this.lastName = lastName;
        this.firstName = firstName;
    }

    var john = new Person('John', 'Doe');

It's also possible to add to this object's prototype (see 'prototype chain' - in js inheritance is from other objects, not a class):

    Person.prototype.greet = function() {
        // use 'this' so that we refer to the object and not some variable (so don't use an arrow function here)
        console.log('hello ' + this.firstName + ' ' + this.lastName);
    }

The naming here is a little confusing. This prototype is the prototype of any object created from the Person function. It is not the prototype of the constructor function. So all objects created from this function constructor can have access to this greet method.

The js engine will now search down the prototype chain for this method:

    john.greet();


If you ever want to see the prototype for an object use __proto__ :

    // not recommended for produciton code
    console.log(john.__proto__);  // { greet: [function] } 

The es6 version of a function constructor is syntactic sugar. Classes in JS are not the same as they are in other languages.
    
    'use strict';

    class Person {
        
        // on each object
        constructor (firstName, lastName) {
            this.lastName = lastName;
            this.firstName = firstName;
        }

        // on the prototype that each object shares
        greet() {
            console.log('hello ' + this.firstName + ' ' + this.lastName);
        }
    }

    // object created the same way
    var john = new Person('John', 'Doe');

Primitive Value: any value that is not an object. These are passed by value into functions.



]

******************************************

                Node

******************************************


Javascript is quite a high level language. Functionality like memory menagement is abstracted away. Also, because js was created for use in browsers it lacks some other common functionality such as modules and sneding files around. V8 is an open source c++ applicaiton that converts JS into machine code instructions for a variety of common microprocessor arcitectures. It is thus a JS engine, and it also meets all the ECMA script specifications for a JS engine. The advantage of v8 is that it has hooks so that it can be embedded in other c++ applicaitons. Node is such an application. It takes v8 and extends so that JS can be given extra functionality. This allows JS to be used outside of the browser. In particular it gives JS enough funcitonality to be used on a server (the client-server model is common, and the best example is on the internet, which used http to communicate between clients and servers). A server is connected to the internet and offers services.

Some features missing from plan js that are addressed in node:

- better way to organise code into reusable pieces - ie modules
- dealing with files - streams and buffers
- databases
- communicate over the internet
- accept requests and send responses
- deal with work that takes a long time - async methods and event loops. Streams for performance



Modules 
----------------------------------

A module is a reusable block of code whose existence does not accidentially impact other code. Javascript did not orignally have this feature (although ECMAscript 6 spec does require it now). Node does this with 'CommonJS modules'.

CommonJS modules are an agreed upon standard for how code modules should be structured. 

Node supplies a built in function for importing modules called 'require('./filePath')'. 

This will make any code that is invoked within that required file run. However any functions from that file can't just be called. This is to prevent naming collisions.

So to call functions from the required file we have to use 'module.exports' in the required file. 

greet.js:

    // this line runs when greet is required in app.js. You don't even need to assign require to a variable
    console.log('Due to require(), this appears automatically in app.js');

    // this will not run in app.js unless we assign it to module.exports.
    var greet = function() {
        console.log("hello");
    };

    module.exports = greet;

app.js

    // to just get that first line from greet running
    // require('./greet');

    // now we use the idea that the require function returns 
    // module.exports.
    const greet = require('./greet');

    greet();

To explain how modules work in nodejs we first have to look at scope.

Scope: where in code you have access to a particular variable or function. 

Scope example with an IIFE, that shows how JS devs once faked up modules:

    var firstName = 'Jane';

    // the () around the function turn it into an expression that the engine
    // then holds and waits to see what you do with it. We execute it.
    
    (function () {
        var firstName = 'John';
        console.log(firstName);
    })();

    console.log(firstName);

    // john
    // Jane

So firstName in the function is protected from firstName from the global scope.

`require` is a function that takes a path. When you require a module in node, node wraps the code in your module within an IIFE. This protects the variable in a new scope, which is then passed into the v8 engine. module.exports is then returned.

If require() cannot find a file that matches it's string argument it then looks for a folder of the same name. Then it will look for an index.js file within that folder. So you can require a number of files in the index.js, and include there return values in one module.exports object. Then, from say app.js, just require the folder that index.js lives in. You can run functions from any of the original files from a single object. 

If `require()` is given the path of a JSON file (wich is really just a text file in JSON format - no methods and property names/keys in quotes), then that JSON string is returned back as a JSON object. 

Common Module Patterns


When node begins parses through a module, module.exports is first created as an empty object. So if you have just a function you are returning then just assign the function definition right to module.exports :

greet.js:

    module.exports = function() {
        console.log('hello');
    }

app.js:

    var greet = require('./greet');
    greet();


Also, because require is returning back our module.export as an object, often people use dot notation.

app.js:

    var greet2 = require('./greet').greet;
    greet2();


We can also employ other ways to make objects such as a function constructor.

greet.js:

    function Greetr() {
        this.greeting = 'Hello World!';
        this.greet = function() {
            console.log(this.greeting);
        }
    }

    // replace the empty object with our own object
    module.exports = new Greetr();

app.js:

    const greet = require('./greet');
    greet.greet();


However, we have to be careful when generating what looks like two different objects:

    const greet1 = require('./greet');
    greet1.greeting = 'Changed Hello World'

    const greet2 = require('./greet');
    greet2.greet();  // Changed Hello World


Here, eventhough it looked like we 'newed up' two different objects, the variables actually hold a reference to the same object. `require` actually caches a return object for each file name.

An alternative, and 4th pattern, is to just place a reference to the constructor function in module.exports and let the `new` happen in app.js:

    module.exports = Greetr;

app .js:

    const Greetr = require('./greet');
    const greet1 = new Greetr();
    greet1.greeting = 'Changed Hello World';

    const greet2 = new Greetr();
    greet2.greet();  // 'Hello World!


The next pattern is very popular and useful:

greet.js:

    var greeting = 'Hello world!';

    function greet() {
        console.log(greeting);
    }

    module.exports = {
        greet: greet
    }


app.js:

    const greet = require('./greet').greet;
    greet();  // 'Hello world!'

So we no longer have access to the greeting variable in greet.js, but we can still see it being applied. We essentialy have a private variable. This is called the 'revealing module pattern': exposing only the properties and methods you want via a returned object. This is a common and clean way to structure and protect code within modules.


Exports vs module.exports.

In your module you can actually use 'exports' instead of module.exports (exports is the parameter name and module.exports is the argument for the IIFE that is wrapped around the code in your module). Both variables point ot the same object in memory. However, exports is tricky to use. If you use an assignment at the top level of the object then you are creating a new object and thus a new reference. It's a lot easier to never use it.



Requiring Native (Core) Modules

Native modules come from the lib folder in node. They are the core of the JS side. You can see all the native modules in the node API. Some need to be explicitly imported, ie `required1 and some do not. You can look in lib to see the file name and work out the path string you will need. However, you don't need a full file path. Just the filename will do. Eg for the Utilities module:

    var util = require('util'); // for libs/util.js

If you happen to have one of your own modules with the same name then using the path separator fill fix the problem: require('/util'). But this is not reccommend anyway since it may lead to confusion.

Some of these modules are wrappers for c++ code and some are just js that you could have written yourself (but why bother?). Here we use the utilities module to date stamp, format and log a variable:

    const util = require('util');
    var name = 'Tony';
    var greeting = util.format('Hello, %s', name);
    util.log(greeting); // 13 Dec 15:28:09 - Hello, Tony 



Modules and ES6

ES6 modules are also suported in v8 (but not node). Eg:

greet.js:

    export function greet() {
        console.log('Hello');
    }

app.js:

    import * as greetr from 'greet';
    greetr.greet()



Events and the Event Emitter
---------------------------------------

Many core node js modules are built upon this concept.

Events in nodejs are:

    'Something that has happened in our app that we can repsond to'.

In nodejs we actually talk about two different kinds of events. On one side we have system events. They come from the c++ side of the nodejs core thanks to a library called libuv. These are events that have come form the computer system, like "I've finished reading a file", or "I've recieved information from the internet". These are events that JS did not originally have. 

On the other side, in the JS core, we have custom events. These are completly different and are events that you can create for yourself. They are created in  an area is called the 'Event Emitter'. 

Sometimes system events are sent on as custom events, so that they appear to be the same thing. But, they are not. 

The js side is actually faking it. They are not real events. JS has no event concept or object. We create our own event library with the techinique that the node event emitter uses. 

One key to understanding the event emiter is recalling that I can use string to dynamically access changing properties in an object:

    var obj = {
        greet: 'Hello'
    }
    // dot notation using the property name / key
    console.log(obj.greet);  // Hello

    //square bracket using the property name / key
    console.log(obj['greet']) // Hello

    // square brackets using a variable to hold the property name
    var prop = 'greet';
    console.log(obj[prop]); // Hello


You also need to recall that array are collections, and since functions are first class objects they too can be included in an array.

    let arr = [];

    arr.push( () => {
        console.log('1');
    });

    arr.push( () => {
        console.log('2');
    });

    arr.push( () => {
        console.log('2');
    });

    arr.forEach((fn) => {
        fn();
    })

    // 1
    // 2
    // 3

In this emiter example we will build our own simple version of the node event emitter. We will be able to see that an event has happened and then repond to it. 

Inside the ee we will have an `on` method. 'on' is a common name for an event listener because it reads nicely. EG I might say 'on a file being opened' or 'on a message being recieved'.

The on method then takes two arguments. One is for the type of event and the other is an event listener.

'Event Listener': code that responds to an event. In JS the event listener code is usually a function. When the event happens this code is invoked. You can have more than one listener for the same event. 

emitter.js:

    // could also use a class contructor to make this object
    // we want to be able to create multiple emitters.
    function Emitter() {
        this.events = {};
    }

    // add an "on" method to the protoype of all objects created from the function constructor
    Emitter.prototype.on = function (type, listener) {

        //if the type property already exists then good, otherwise make a new array.
        this.events[type] = this.events[type] || [];
        
        // now push the listener function into the array
        // we are building up an array of functions. One array for each event type.
        // onBlah: [function() {...}, function() {...}, ... }
        this.events[type].push(listener);
    }

    // now we want to say that something happened; we emit an event.
    Emitter.prototype.emit = function (type) {
        // If I have the type of event on my object, i'll loop over the 
        // associated array and execute each listener function
        if(this.events[type]) {
            this.events[type].forEach(listener => {
                listener();
            })
        }
    }

    module.exports = Emitter;



app.js:

    const Emitter = require('./emitter');

    let emtr = new Emitter();

    // Add some listeners

    // say whenever a greet happens we want to do something.
    emtr.on('greet', () => {
        console.log('Somewhere, someone said hello.');
    });

    // And this is another thing to do when a greet occurs
    emtr.on('greet', () => {
        console.log('A greeting occured!');
    });

    // simulate the event:
    console.log('Hello!');

    // let the application know that a greet event happened
    emtr.emit('greet');

You can check out the real event framework in lib/events.js. 'On' is an alias for `addEventListener`.

To re-write our example so that it uses the real event framework just change the import statement in app.js:

    const Emitter = require('events');

TO prevent magic strings we also create and require a config.js file:

config.js:

    module.exports = {
        events: {
            GREET: 'greet',
            FILESAVED: 'filesaved',
            FILEOPENED: 'fileopened'
        }
    }


Magic String: a string that has some special meaning in our code. They make it easy for a typo to cause a bug, and hard to track down that bug. It's just a string, not a variable. Relying on a string to be the basis for the logic in your code is dangerous.


So out final app.js is:

    const Emitter = require('events');
    const events = require('./config').events;

    let emtr = new Emitter();

    // Add some listeners

    // say whenever a greet happens we want to do something.
    emtr.on(events.GREET, () => {
        console.log('Somewhere, someone said hello.');
    });

    // And this is another thing to do when a greet occurs
    emtr.on(events.GREET, () => {
        console.log('A greeting occured!');
    });

    // simulate the event:
    console.log('Hello!');

    // let the application know that a greet event happened
    emtr.emit(events.GREET);


Aside on Object.create()

Object.create takes an object. It then makes a new object and set the argument as the new object's prototype.

    const Person = {
        firstName = "",
        lastName = "",
        greet = function() {
            console.log('Hello ' + firstName + ' ' + lastName);
        }
    }

    // john has Person as a prototype but it creates it's own name variables
    // so that the JS engine is not looking for them up the chain
    let john = Object.create(Person);
    john.firstName = "John";
    john.lastName = "Doe";

    // jane share the same prototype as john
    let jane = Object.create(Person);
    jane.firstName = "Jane";
    jane.lastName = "Doe";


Inheriting from the Event Emitter

We know about several inheritance approaches (ie ways to set up the prototype chain). Function constructors, extending a class and Object.create().

Within util.js (the utilities module in the node library), there is a method called inherits. This allows us to give set up an entire node library object, like the event emitter module, as the prototype of one of our own contructors. That is, a node module becomes the prototype of a constructor. The first argument is your custom constructor and the second is the module that will become the prototype.


Under the hood it looks a little like 

    myConstructor.prototype = Object.create(superContructor.prototype);


app.js:

    var EventEmitter = require('events');
    var util = require('util');

    function Greetr() {
        this.greeting = 'Hello World!';
    }

    // The prototype for objects created from EventEmitter will also be the prototype of the prototypes
    // created for all objects created from Greetr
    util.inherits(Greetr, EventEmitter);

    // You can still as your own methods and properties
    Greetr.prototype.greet = function(data) {
        console.log(this.greeting + ': ' + data);

        // And you can access EE methods too (which are on 'this' - 
        // the created object- don't call EventEmitter.emit because
        // EventEmitter is a constructor and not your object)
        // Here we use an optional argument in emit. Emit automatically
        // passes extra arguments into the event listeners
        this.emit('greet', data);  // normally don't use a magic string
    }

    const greeter1 = new Greetr();

    greeter1.on('greet', function(data) {
        console.log('Someone greeted: ' + data);
    })

    greeter1.greet('Tony');

Here is the same code converted to es6 (the call to super is discussed further below).

greetr.js:

    'use strict';

    var EventEmitter = require('events');

    // use `extends` instead of util.inherits(Greetr, EventEmitter);
    module.exports = class Greetr extends EventEmitter {
        constructor() {
            super(); // get specific object properties
            this.greeting = 'Hello World!';
        }

        greet(data) {
            console.log(this.greeting + ': ' + data);
            this.emit('greet', data);  // normally don't use a magic string
        }

    }

app.js

    var Greetr = require('./greetr');

    const greeter1 = new Greetr();

    greeter1.on('greet', function (data) {
        console.log('Someone greeted: ' + data);
    })

    greeter1.greet('Tony');



This is a key concept in node. Many objects in node can do many things plus listen for and emit events.

Here is another example that waits for async random number requests to be generated.

numEvents.js:

    // extends the events lib to handle random number events
    const util = require('util');
    const EventEmitter = require('events');
    const eventConfig = require('./config2').events;

    function NumEventer() {
        // add in a call to the super constructor if you also need some 
        // properties generated only in that constructor
        // EventEmitter.call(this);
    }

    const numRequest = function() {
        setTimeout(() => {
            const randomNum = Math.random();
            this.emit(eventConfig.NUM_REC, randomNum);
        }, 2000);
    }

    util.inherits(NumEventer, EventEmitter);

    NumEventer.prototype.on(eventConfig.NUM_REC, function(numData) {
        console.log(`Your percentage is: ${(numData*100).toPrecision(2)} %`);
    });

    NumEventer.prototype.request = function() {
        numRequest.call(this);
        console.log('Sent a request for random number');
    };

    module.exports = NumEventer;

app.js

    const NumEventer = require('./numEvents');
    const numEventer = new NumEventer();

    // request a random number from the "net"
    numEventer.request();


Here is another example of utils along with comlete inheritance. This a sommon pattern in nodejs

    const util = require('util');

    function Person() {
        this.firstName = "John";
        this.lastName = "Doe";
    }

    Person.prototype.greet = function() {
        console.log(`Hello ${this.firstName} ${this.lastName}`);
    }

    function PoliceOfficer() {
        // Because this is a function constructor an empty object is assigned to 'this'
        Person.call(this); // attach properties to `this` from the super constructor
        this.badgeNumer = 1234;
    }

    // make the prototype of the prototype for police objects the same prototype
    // that Person objects have.
    util.inherits(PoliceOfficer, Person);

    const officer = new PoliceOfficer();

    // name properties inherited
    officer.greet(); // Hello John Doe

    // greet is on the proto of the proto!
    officer.__proto__.__proto__.greet(); // Hello undefined undefined



Aysnchronous Code, Streams and Buffers
-------------------------------------------

Aysnchronous: more than one process is running simultaneously. JS and v8 are synchronous (one process or one line of code running at a time). Node does things asynchronously. When we think about synchronisity we really have to think about what a js engine is sitting inside of. It is a browser or is it node?

First recall what a callback is: a function passed to another function, which we asusme will be invoked at some point. The function 'calls back' invoking the function you gave it when it is done with its work.

We now look at the c++ side of events in node with libuv, with the event loop and non-blocking code. We have already seem custom events with the event emitter. This was just a trick. Real system events are handled in the c++ core by a library called libuv. This manages events from the operating system. It is closer to the machine. 

libuv sends requests to the operating system. Examples of these include opening files, database access, and requesting data from the internet. Responses back from the OS are placed in a queue of events within libuv. libuv constantly checks to see if something is in the queue. Sometimes, between checks, multiple events may have been added to the queue. When libuv does see that something is complete it processes it by running the associated callback back back in v8. This is code that is meant to run when the libuv event loop completes (request --> event to queue --> event processed). If you dig into the c code within linuv's core code you actually see a while loop.  

V8 is synchronous, so any work related to the callback is again placed in v8's own queue of work. However, this entire process is aysnchronous. libuv is running it's own event loop while v8 is carrying on with whatever work it has to do. So this why node is often referred to as 'non-blocking'; node can get on with other work while waiting for some operation to complete. All your javascript code keeps working, and you app does not freeze up, while you wait for a request. 


Streams and Buffers

A buffer is a temporary holding place for data while that data is being moved from one place to another. It is intentioanlly limited in size so that the temporary data is not stored for long. Usually this data is handled by an object called a 'stream'.

A stream is a sequence of data moving over time. Within it are pieces of data that are eventually combined into a whole. 



Buffers

Traditionally JS was not very good at encoding. That is, dealing with pure binary data. Suck work was more related to server side jobs. So node expands JS and allows us to deal with binary data.

Binary data: data is stored in binary (aka base 2). Each 1 (high V) or 0 (low V) is called a 'bit' or 'binary digit'.

Character set: a representation of characters as numbers. Each character gets a number. Unicode and ASCII are character sets.

Character encoding: how characters are stored in binary. The numbers (or 'code points') are converted and stored in binary numbers.

UTF-8 is a popular character set because it encodes into 8 bit numbers, which allows for many characters.

The node API details a built in buffer object. It mas many methods for dealing with binary data. You can choose a number of encoding syles.

Ultimately a node buffer object holds raw binary data, but it allows us to change what you see with encoding. 

    // buffer is so fundamental that a 'require' is not necessary
    var buffer = new Buffer('Hello', 'utf-8'); // encode the given string with utf-8

    console.log(buffer);   // <Buffer 48 65 6c 6c 6f>  - more readable hex representation of the binary data
    console.log(buffer.toString());  // Hello is produced with utf8.
    console.log(buffer.toJSON());  // { type: 'Buffer', data: [ 72, 101, 108, 108, 111 ] } - the buffer is an object
    console.log(buffer[1]); // 101 - the unicode decimal number for e. 

    // overwrite the buffer, which was initialised with only five characters
    buffer.write('wo');
    console.log(buffer.toString()); // wollo

Often you won't directly deal with the buffer. Mostly a buffer object is coming back from some other utility or feature in nodejs. 


Typed arrays;

This is a feature embedded into the v8 engine. It is not actually nodejs. It is a way to deal with binary data in the buffer. If you change the array you are changing the buffer. If you read from the array you actually reading from the buffer. 

These typed array have various structures (or types!). However, you use them like an array.

    // storing raw binary data: 64 bits
    var buffer = new ArrayBuffer(8); // give the size in bytes, not bits

    // here we choose the int32 type
    var view = new Int32Array(buffer);

    view[0] = 5;
    view[1] = 15;
    console.log(view);  // Int32Array [ 5, 15 ]

    // our 8 byte buffer does not enough capacity for another number
    view[2] = 30;
    console.log(view);  // Int32Array [ 5, 15 ] - no change



Aside on callbacks and databases:

You might be collecting information from a database, but, depending on the situation, you might also like to do different things with that data. One approach is to pass in callbacks that take in the same response data but deal with it in different ways.

    function greet(callback) {
        console.log('hello');
        var data = {
            name: 'John Doe'
        }
        callback(data);
    }

    function logIt(data) {
        console.log('The callback was invoked');
        console.log(data);
    }

    function greetName(data) {
        console.log('A different callback was invoked');
        console.log(data.name);
    }

    greet(logIt);

    greet(greetName);

    // hello
    // The callback was invoked
    // { name: 'John Doe' }
    // hello
    // A different callback was invoked
    // John Doe


Aync code example with files and the fs module (file system):

readFileSync reads a text file and returns a string. Internally it loads the contents of a file into a buffer. You can specify the encoding in which you would like to get your string back too. 

Recall that __dirname is a argument made for the IIFE that wraps a module.  It gives the path to the directory in which we are running code.

greet.txt:

    Hello world!

app.js:

    const fs = require('fs');
    const greet = fs.readFileSync(__dirname + '/greet.txt', 'utf-8'); // utf-8 encoding specified
    console.log(greet);  // Hello world!

Notice 'Sync' in the method name. This tells that all code waits until the file has been read to the buffer and then returned back as a string. In most case you do not want your code to be synchronous. For example you might have many users and you do not want them to be blocked. The file might be vary large and so require a large buffer or a long stream, which takes time. fs.readFile is async and requires a callback. As in the normal event-loop case, the callback is run once libuv has recieved a response from the OS. 

note: 'error first callbacks' are popular in node. The first argument will be null if there is no error. 

    const greet2 = fs.readFile(__dirname + '/greet.txt', function(error, data) {
        console.log('data inside readfile callback: ', data);
    });

    console.log(greet2);

    // undefined - the 2nd log ran before the data was ready
    // data inside readfile callback:  <Buffer 48 65 6c 6c 6f 20 77 6f 72 6c 64 21>  // binary buffer data

So we saw the binary data in the buffer (logged as hex to make it more readable), but we can see this as a string by using toString(). And while utf-8 encoding is the default we'll specify it again:

    const greet2 = fs.readFile(__dirname + '/greet.txt', 'utf-8', function(error, data) {
    console.log('data inside readfile callback: ', data.toString());
    });

    console.log(greet2);

    // undefined
    // data inside readfile callback:  Hello world!

Allways use async when you can because it is a better experience for your users. 



Streams

The buffer lives in a place called the heap. This is some memory allocated to the program. If you had many users all requesting the same file at the same time you may have problems.

Chunk: a piece of data being sent through a stream. Usully limited to the buffer size.

If you dig into the stream.js library file you can see that a stream object is an event emitter (it ueses util.inherits to extend EE). When certain events have occured code is then run. There are 5 types of stream objects you can use directly from node; readable (only read the data and you can't send data back), writable (send data but you can't read data from the stream), duplex (read and write data), transform (change the data as it moves through the stream), and passthrough. 

The shared stream object at the base of these stypes is an 'abstract' or base class. Abstract classes are those you never work with directly but instead always only inherit from into a new custom object. This means that we are provided with base ideas about what a stream should be but we have to implement how exacty how we deal with the information we expect to recieve. In saying this however, nodejs provides several implementations, which were those mentioned above.

So in terms of a protoype chain we might have something like 

    EventEmitter.prototype --> Stream.prototype --> Stream.Readable.prototype --> CustomStream

When we talk about readable and writable we have to remember we are talking about these concepts from node's perspective. There may be another entity doing it's own thing at the other end of a data stream (as opposed to a stream object), such as a webserver or a browser. For example a node server will perceive that requests from a browser are readable but return data that is send back is writable.

Since streams are abstract we need a concrete implementation as an example. One is the fs module. fs.readStream. This is a specialised type of read only stream object which follows the above flow diagrammme.

So in this example there is a text file, greet.txt, which has 10's of kBytes of data. Once files get larger they get bigger than the buffer you'll only get pieces, or 'chunks' of the text file at any one time. This means that same 'data' event will be fired for each chunk, so that each associated event listener will be invoked multiple times. The default buffer size is 64 kBytes.

    const fs = require('fs');

    var readable = fs.createReadStream(
        __dirname + '/greet.txt',
        { 
            encoding: 'utf8',          // will mean that a console.log is a string and not hex
            highWaterMark: (32*1024) // override the default with 32 kBytes.
        }
    );

    // check out the docs for the correct event type; 'data'
    readable.on('data', function(chunk) {
    
        // If no encoding is specified this is a hex representaiton of the binary.
        // Each hex number is then a byte summary so that the length is the byte count.
        // console.log(chunk);

        console.log(chunk.length); // will be less that of the buffer's size in bytes (32 kBytes here)
    });

    // 32768
    // 9084


Now let's write the data to another file.

    const fs = require('fs');

    var readable = fs.createReadStream(
        __dirname + '/greet.txt'
    );

    // create a writable stream object
    var writable = fs.createWriteStream(__dirname + '/greetCopy.txt');

    readable.on('data', function(chunk) {
        writable.write(chunk);
    });

[now we have seen several ways of reading data from a file; readFileSync, readFile, and createReadStream]



Pipes

Reading and then writing, as in the previous example, is such a common pattern that there is actually a common term for it; 'pipes'.

Pipes: A pipe connects two streams by writing one stream with what is being read from another stream. In node you pipe from a readable stream to a writabe stream.

We can attach several pipes together and send data to multiple writable streams. 

Readable objects have a pipe method attached: it looks like function pipe (destination, pipeOptions) {...}, where destonation is a writable stream. Under the hood, it's just a more robust version of what we did above. This function also returns the destination writable stream. This helps us to write our code in a cleaner way.

    const fs = require('fs');

    var readable = fs.createReadStream(
        __dirname + '/greet.txt'
    );

    // create a writable stream object
    var writable = fs.createWriteStream(__dirname + '/greetCopy.txt');

    readable.pipe(writable);  // [event listener created for us]

Now, if the streams are duplex (readable and writable), then we can use the return value to work with multiple streams. 

Here we will use zlib, which is part of the node core. This allows us to implement a gzip file (a common file compression algorithm). We will copy the contents as before but also create a compressed verion. Here we use zlib.createGzip(); to create a duplex stream. Since that stream is thus also writable, the pipe method will return a stream of compressed data, which can then be writen to file.  This is called chaining.

Chaining: a method returns an object so that we can keep calling methods. If the parent object is returned then it's called cascading.

    const fs = require('fs');
    const zlib = require('zlib');

    var readable = fs.createReadStream(
        __dirname + '/greet.txt'
    );

    var writable = fs.createWriteStream(__dirname + '/greetCopy.txt');

    // a stream of compressed data to a file
    var compressed = fs.createWriteStream(__dirname + '/greetCopy.txt.gz');

    // a transformative stream; creates compressed data
    var gzip = zlib.createGzip(); 

    // Cannot chain from 'writable' because it's not readable
    readable.pipe(writable);

    // three streams chanied together here: 'readable', 'gzip', 'compressed'.
    readable.pipe(gzip).pipe(compressed);


Node's memory and speed is optimised for streams, so we should always be thinking about when we can use streams. Always consider streams and async methods over sync methods.












